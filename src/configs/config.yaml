
# pipe = pipeline("text-generation", model="TinyLlama/TinyLlama-1.1B-Chat-v1.0", torch_dtype=torch.bfloat16, device_map="auto")
model:
  task: "text-generation"
  name: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
  torch_dtype: torch.bfloat16
  device_map: "auto"

data:
  dataset: "GMSK8"
  max_length: 512
  train:
    path: "data/train.jsonl"
  test:
    path: "data/test.jsonl"


# GRPO Training Parameters
num_epochs: 10
K: 8                      # number of responses per question for GRPO
ckpt_path: "checkpoints/ckpt.pt"

# Hybrid Reward Weighting
alpha: 0.5                # TODO: tune - weight for verifier score component
beta: 0.5                 # TODO: tune - weight for RM score component
eps: 0.01                 # small constant for numerical stability in normalization

# HuggingFace Trainer config (not used by GRPO loop)
training:
  output_dir: "./tinyllama-finetuned"
  num_train_epochs: 1
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  learning_rate: 2e-5
  warmup_steps: 50
  logging_steps: 50
  eval_steps: 100
  save_steps: 100
  save_total_limit: 2
  bf16: true  # or fp16: true if your GPU supports it
  report_to: "none"